import { AlibabaTongyiEmbeddings } from '@langchain/community/embeddings/alibaba_tongyi'
import { Chroma } from '@langchain/community/vectorstores/chroma'
import { ChromaClient } from 'chromadb'
import { Document } from '@langchain/core/documents'
import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters'
import dotenv from 'dotenv'
import path from 'path'
import { promises as fs } from 'fs'

dotenv.config()

const VECTOR_STORE_URL = 'http://localhost:8000'
const COLLECTION_NAME = 'wangEditor-doc'
const SOURCE_FILE_NAME = 'wangEditor-doc.md'

const chromaClient = new ChromaClient({
  path: VECTOR_STORE_URL,
})

async function loadMarkdownFile(filePath) {
  console.log(`ğŸ“„ æ­£åœ¨åŠ è½½ Markdown æ–‡ä»¶: ${filePath}`)
  const content = await fs.readFile(filePath, 'utf-8')
  console.log(`âœ… æ–‡ä»¶åŠ è½½å®Œæˆï¼Œå­—ç¬¦æ•°: ${content.length}`)
  return content
}

async function splitMarkdownContent(content) {
  console.log('âœ‚ï¸ å¼€å§‹åˆ†å‰²æ–‡æœ¬...')
  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: 1000,
    chunkOverlap: 200,
    separators: ['\n\n', '\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', '.', ' ', ''],
  })

  const baseDocument = new Document({
    pageContent: content,
    metadata: { source: SOURCE_FILE_NAME },
  })

  const documents = await splitter.splitDocuments([baseDocument])
  const documentsWithIndex = documents.map(
    (doc, index) =>
      new Document({
        pageContent: doc.pageContent,
        metadata: { ...doc.metadata, chunk_index: index },
      })
  )

  console.log(`âœ… æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ ${documentsWithIndex.length} ä¸ªç‰‡æ®µ`)
  return documentsWithIndex
}

async function clearExistingCollection() {
  console.log(`ğŸ§¹ æ¸…ç†é›†åˆ ${COLLECTION_NAME}...`)
  try {
    await chromaClient.deleteCollection({ name: COLLECTION_NAME })
    console.log('âœ… å·²åˆ é™¤æ—§çš„é›†åˆ')
  } catch (error) {
    if (error.message?.includes('NotFound') || error.message?.includes('does not exist')) {
      console.log('â„¹ï¸ ç›®æ ‡é›†åˆä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤æ­¥éª¤')
    } else {
      console.warn(`âš ï¸ åˆ é™¤é›†åˆæ—¶å‡ºç°éè‡´å‘½é”™è¯¯: ${error.message}`)
    }
  }
}

async function storeDocuments(documents) {
  console.log('ğŸ§  æ­£åœ¨ç”Ÿæˆå‘é‡å¹¶å†™å…¥ Chroma...')
  const embeddings = new AlibabaTongyiEmbeddings({})

  const vectorStore = await Chroma.fromDocuments(documents, embeddings, {
    collectionName: COLLECTION_NAME,
    url: VECTOR_STORE_URL,
    collectionMetadata: {
      source: SOURCE_FILE_NAME,
      chunk_size: 1000,
      chunk_overlap: 200,
    },
  })

  console.log(`âœ… å·²æˆåŠŸå†™å…¥ ${documents.length} æ¡å‘é‡æ•°æ®`)
  return vectorStore
}

async function runSmokeTests(vectorStore, expectedCount) {
  console.log('\nğŸ§ª å¼€å§‹è¿è¡Œç®€å•æµ‹è¯•...')

  const collection = await chromaClient.getCollection({ name: COLLECTION_NAME })
  const count = await collection.count()

  if (count !== expectedCount) {
    throw new Error(`é›†åˆå†…æ–‡æ¡£æ•°é‡ä¸º ${count}ï¼Œä¸é¢„æœŸçš„ ${expectedCount} ä¸ä¸€è‡´`)
  }
  console.log(`âœ… æ–‡æ¡£æ•°é‡æµ‹è¯•é€šè¿‡: ${count} æ¡è®°å½•`)

  const sample = await collection.get({
    limit: 1,
    include: ['documents', 'metadatas'],
  })
  const firstDocument = sample.documents?.[0]?.[0]

  if (!firstDocument) {
    throw new Error('æ— æ³•ä»é›†åˆä¸­è¯»å–ç¤ºä¾‹æ–‡æ¡£')
  }
  console.log(`âœ… ç¤ºä¾‹æ–‡æ¡£è¯»å–æˆåŠŸï¼Œé•¿åº¦: ${firstDocument.length}`)

  const testQueries = ['wangEditor åŠŸèƒ½', 'å¯Œæ–‡æœ¬ ç¼–è¾‘å™¨', 'å›¾ç‰‡ ä¸Šä¼ ']
  for (const query of testQueries) {
    const results = await vectorStore.similaritySearch(query, 2)
    if (results.length === 0) {
      throw new Error(`æŸ¥è¯¢ "${query}" æœªè¿”å›ä»»ä½•ç»“æœ`)
    }
    console.log(`ğŸ” æŸ¥è¯¢ "${query}" è¿”å› ${results.length} æ¡ç»“æœï¼Œé¦–æ¡ç‰‡æ®µé•¿åº¦: ${results[0].pageContent.length}`)
  }

  console.log('ğŸ§ª æµ‹è¯•å…¨éƒ¨é€šè¿‡\n')
}

async function resolveMarkdownPath() {
  const candidates = [
    path.resolve(process.cwd(), 'files', SOURCE_FILE_NAME),
    path.resolve(process.cwd(), 'files', 'files', SOURCE_FILE_NAME),
    path.resolve(process.cwd(), SOURCE_FILE_NAME),
  ]

  for (const candidate of candidates) {
    try {
      await fs.access(candidate)
      return candidate
    } catch (error) {
      // continue trying next candidate
    }
  }

  throw new Error(`æœªæ‰¾åˆ° Markdown æ–‡ä»¶ï¼Œå°è¯•è·¯å¾„:\n${candidates.join('\n')}`)
}

async function main() {
  try {
    const markdownPath = await resolveMarkdownPath()
    const content = await loadMarkdownFile(markdownPath)

    const documents = await splitMarkdownContent(content)
    await clearExistingCollection()

    const vectorStore = await storeDocuments(documents)
    await runSmokeTests(vectorStore, documents.length)

    console.log('ğŸ‰ wangEditor æ–‡æ¡£å‘é‡åŒ–æµç¨‹å·²å®Œæˆ')
  } catch (error) {
    console.error(`âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: ${error.message}`)
    if (error.message.includes('ECONNREFUSED')) {
      console.error('æ— æ³•è¿æ¥åˆ° Chroma æœåŠ¡ï¼Œè¯·ç¡®è®¤ http://localhost:8000 å·²å¯åŠ¨')
    }
    process.exit(1)
  }
}

main()